\documentclass{article}
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\begin{document}
\section{Winners-losers models}
In the Winners-Losers model, after an interaction, both the agents have
to change their state.

The algorithm of the Winners-Losers model is as follows:

\vskip2mm
\noindent\textbf{Model 1}
\vskip1mm
\hrule

\begin{itemize}
\tightlist
\item
  create a population of agents
\item
  initialize their money holding to a given amount (say 1)
\item
  at each time

  \begin{itemize}
  \tightlist
  \item
    two agents are randomly drawn from the population
  \item
    they pool their money holdings
  \item
    the pooled money is reassigned to the two agents giving a random
    share to the first agent and the rest to the other agent.
  \end{itemize}
\end{itemize}
\vskip-2mm
\hrule

\vskip4mm
This dynamics is very simple, but we can have some extreme cases. For
example, an agent can be never chosen.

If we want to avoid these cases, we can modify our main loop as follows

\vskip2mm
\noindent\textbf{Model 2}
\vskip1mm
\hrule

\begin{itemize}
\tightlist
\item
  at each time step

  \begin{itemize}
  \tightlist
  \item
    each agent

    \begin{itemize}
    \tightlist
    \item
      choose randomly another agent
    \item
      the two agents pool and share the money as before
    \end{itemize}
  \end{itemize}
\end{itemize}

\vskip-2mm
\hrule

\vskip4mm

In this implementation, each agent has at least an exchange at each time
step. Some agents can have multiple exchanges if it is chosen by
different agents.

In a more complicate variant, a choosing agent can observe a number of other agents an choose one of them according to some criterium. The main loop can be as follows

\vskip2mm
\noindent\textbf{Model 3}
\vskip1mm
\hrule

\begin{itemize}
\tightlist
\item
  at each time step

  \begin{itemize}
  \tightlist
  \item
    each agent

    \begin{itemize}
    \tightlist
    \item
      choose a set of other agents
    \item
      choose one agent in the set according to some preference criterion
    \item
      the two agents pool and share the money as before
    \end{itemize}
  \end{itemize}
\end{itemize}

\vskip-2mm
\hrule

\vskip4mm

The model is so simple that its straightforward implementation does not require an agent-based approach. One could create an array of money holdings, take two of them at each iteration and update them according to the rule seen above.

Because we are interested in agent-based models, we will proceed with the agent-based implementation as a tutorial towards more advanced models.  

Among the many available platforms we will use the Repast suite.

\section{The Repast suite}

Repast provides facilities to run agent-based simulation.

It comes in three different toolkits. 
The classic one is ``Repast Simphony'', a java-based toolkit designated for use in personal computer and small clusters.  

The increase in computer availability brought the development team to release the ``Repast for High Performance Computing'' toolkit, that is a ``C++-based distributed agent-based modeling toolkit that is designed for use on large computing clusters and supercomputers.'' 

The most recent toolkit is ``Repast for Python''. ``It is a Python-based distributed agent-based modeling toolkit to apply large-scale distributed ABM methods.'' 

All the mentioned Repast toolkits provide facilities to implement agent-based models such as:
\begin{itemize}
\tightlist
	\item a scheduler to controls the dynamic of the simulation
	\item spatial structures such as grids or continuous 2d space to locate agents
	\item network structures
	\item \ldots
\end{itemize}

We plan to use ``Repast for Python'' that is the most up-to-date toolkit in the Repast family. In addition to the previous mentioned facilities, it builds some of its functions on mpi4py to ease the management of parallel computation.

In the following section we will deal with the following implementations of winners-losers model 1:
\begin{itemize}
	\item WL model without agents coded in python (without Repast functions)
	\item WL model with agents in a no parallel setting (Using Repast facilities excluded those for parallel computation)
	\item WL model with agents in a parallel setting (Using repast facilities excluded those for parallel computation. The parallel computation is implemented using mpi4py functions)
	\item WL model with agents in a parallel setting (Using repast facilities including those for parallel computation.)
\end{itemize}

\section{Model 1 without agents}
\subsection{Implementation}

The implementation can be performed without agents.

\section{Model 1 parallel without ghosts}

We will describe hereafter how to parallelize the various version of the
WL model using Repast4py.


In the basic version, we have an interaction for each time tick. The
closer exercise to the basic model in a parallel setting is to make subsequent interaction
to be performed on different processes (or ranks). Although this is not
a true parallelization, it will serve to understand communication among
ranks.

Algorithm

\begin{itemize}
\tightlist
\item
  at each time tick

  \begin{itemize}
  \tightlist
  \item
    choose a rank (say Ra)
  \item
	  Ra chooses a rank (say Rb, note that Rb can be equal to Ra. In this case we will have a local interaction)
  \item
    Ra choose an agent (say ARa)
   \item
	   ARa builds a reference to an agent in Rb (say ARb)
  \item
    if Ra == Rb the interaction is local
    \begin{itemize}
		    \tightlist
    \item
	    check that ARb and ARa are different, if not change the second agent with a different one
    \item
      pool the two agents' money holdings and reallocate the sum randomly
    \end{itemize}
  \item
    else

    \begin{itemize}
    \tightlist
    \item
	    Ra informs all other ranks that he selected Rb, its rank number and that the interaction is not local (MPI send)
    \item
	    all other ranks receive the new (MPI receive)
    \item
	    Rb discover he was chosen and sets the chosen flag to allow point-to-point communications
	    
	    Rb gets ARb and extract its money holdings
      %ra requests the agent to rb (the ghost of the agent is created in rank1 say GAR2inR1)
    \item
	    Rb send to Ra the info on ARb's money holdings (MPI send)
%      AR1 interact with GAR2inR1, AR1 changes its variable and take note of AR2 variable
    \item
	    Ra receive the info (MPI receive)

	    Ra pool resources and reallocate them

	    Ra updates ARa money holdings
    \item
	    Ra send information on ARb new money holding
    \item
	    Rb receive the information (MPI receive)

	    Rb update ARb's money holding
    \end{itemize}
  \end{itemize}
\end{itemize}

An implementation of the algorithm is provided in the python script \verb+01_wl.py+.

The command
\verb+mpirun -n 4 python 01_wl.py+ executes the model in 4 ranks.
By inserting several print statements, we can verify how the code works on the 4 ranks. The output from time tick 1 and 2 is reported hereafter.


\footnotesize
\begin{verbatim}
RANK 1                                     RANK 2
-- tick 1 ----------------------------------------------------------------------------
updating rank 1 at tick 1 active rank 1
local interaction in rank  1:          
      agent  2  interacts with agent  0    tick 1 active r 1 self r 2 
                                                    received data ((0, 0, 1), 1, False)
sum of money holdings  2
share  0.06755543644766471
money to local agent  0.135  
    money to second local agent  1.864 
The two local agents money was updated 
-- tick 2 ----------------------------------------------------------------------------
                                           updating rank 2 at tick 2 active rank 2
                                           local agent (3, 0, 2) interact with (2, 0, 1)
tick 2 active r 2 self r 1 
       received data ((2, 0, 1), 2, True)
this agent is in my rank, talking with rank 2
information sent to rank 2 money 0.135
                                           information received from rank 1: money 0.135
                                           sum of money holdings  1.135
                                           share  0.86
                                           money to local agent  0.976
                                                    money to remote agent  0.158
                                           local agent's money updated
Information received from rank 2 money 0.158
agent  (2, 0, 1)  money updated
--------------------------------------------------------------------------------------


RANK 0                                     RANK 3
-- tick 1 ----------------------------------------------------------------------------
tick 1 active r 1 self r 0                 tick 1 active r 1 self r 3 
      received data ((0, 0, 1), 1, False)  received data ((0, 0, 1), 1, False)
-- tick 2 ----------------------------------------------------------------------------
tick 2 active r 2 self r 0                 tick 2 active r 2 self r 3
      received data ((2, 0, 1), 2, True)   received data ((2, 0, 1), 2, True)
--------------------------------------------------------------------------------------
\end{verbatim}
\normalsize

It is interesting to note that agent 2 in rank 1 is involved in both time tick.
In the first time tick he randomly choose agent 0 in the same rank.
In time tick 2 he is selected by agent 3 in rank 2 as a partner for interaction. Therefore rank 1 send information on agent's 2 money holding to rank 2.
Rank 2 performs the computation and send back information on rank 1's agent's money holdings. Finally, rank 1 updates the agent's state.

\section{Model 1 parallel with ghosts}

Till now, in case of interaction between two agents in two different ranks, the ranks send and receive information to allow their agents to update their states.

Repast4py is based on a mechanism that enrich each rank with copies of agents that the rank would had not access to.

A key function for achieving the result is \verb+request_agents()+ belonging to the context module.

The mechanism works as follows:

\begin{itemize}
	\tightlist
	\item 
\verb+request_agents+ sends to the other ranks the ids of agents asked for by a rank
	\item 
The other ranks take the requested agents and ask them to save their current state in a tuple.
	\item 
The collected tuples are send back to the requesting rank
	\item 
		The requesting rank uses the received information to create agents. In particular, the rank creates an agent identical to the one which is in the rank for each received tuple. The newly created agents are called ghosts. And are stored in a list.
	\item 
		Afterwards, the interaction can be performed locally between the agent and a ghost. The ghost is taken from the rank's ghost list.
\end{itemize}

To make this mechanism work, the user has to perform two setups.

First, the user has to endow agents with a \verb+save(....)+ function.
Repast allows each rank to collect information from requested agent. In particular it calls an agents' function called \verb+save(....)+.
Therefore, the user has to endow the agent with the save function. This function could not be implemented by the Repast team because it is model specific (agents are different among models, and the tuple returned by \verb+save+ can be shaped only by the modeler).

The second code integration consist in providing a function accessible at rank level whose role is to create new agents given a tuple with the same structure of \verb+save+ return. This function is the second input argument \verb+request_agent+. The function can be named at the modeler convenience, however, the repast convention is to call it \verb+restore_agent(....)+   


\section{Model 2 parallel implementation with ghosts}
A parallel implementation of model 2 with Repast4py ghosts can be as follows:

\begin{itemize}
\tightlist
\item
  Store information on how many ranks there are and how many agents are
  in each rank
\item
  in each rank at each time step

  \begin{itemize}
  \tightlist
  \item
    create RL1 (an empty list where to store agents to be requested to
    other ranks)
  \item
    each agent

    \begin{itemize}
    \tightlist
    \item
      create AL1 (an empty list where to store ids of agents that will
      be chosen in next 2 steps)
    \item
      randomly draw a rank (or n ranks)
    \item
      randomly draw an agent id belonging to each rank chosen in the
      previous step
    \item
      add agents ids to AL1
    \item
      if rank is different from current rank add id to RL1, but only if
      the id is not yet in RL1 (this is because we will explain below
      that a ghosted agent can be involved at most in an exchange)
    \end{itemize}
  \item
    current rank request RL1. As a result current rank create ghosts and
    other ranks accounts for ghosted
  \end{itemize}
\end{itemize}

Now we have to make sure that each ghost is involved in at most a
transaction. The ghosted list can help in identifying if an agent is
ghosted in more than a rank. We have to delete ghost in such a way that
the agent has only one ghost.

Now exchanges can be performed. Local exchanges can be accounted
immediately exchanges with ghost are registered in a list

The rank send the list and the receiving rank update the agent holding
\end{document}
