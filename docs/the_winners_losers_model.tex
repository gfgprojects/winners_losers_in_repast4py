\documentclass{article}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    allcolors=blue
    }
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\begin{document}
\section{Winners-losers models}
In the Winners-Losers model, agents have only one variable: their money holding. Their holdings are modified as follows. At each iteration two agents meet, pool their money holdings, and share the sum randomly.  
Therefore, after an interaction both the agents have their money holdings changed.


The algorithm of the Winners-Losers model is as follows:

\vskip2mm
\noindent\textbf{Model 1}
\vskip1mm
\hrule

\begin{itemize}
\tightlist
\item
  create a population of agents
\item
  initialize their money holding to a given amount (say 1)
\item
  at each time

  \begin{itemize}
  \tightlist
  \item
    two agents are randomly drawn from the population
  \item
    they pool their money holdings
  \item
    the pooled money is reassigned to the two agents giving a random
    share to the first agent and the rest to the other agent.
  \end{itemize}
\end{itemize}
\vskip-2mm
\hrule

\vskip4mm
This dynamics is very simple, but we can have some extreme cases. For
example, an agent can never be chosen.

If we want to avoid these cases, we can modify our main loop as follows

\vskip2mm
\noindent\textbf{Model 2}
\vskip1mm
\hrule

\begin{itemize}
\tightlist
\item
  at each time step

  \begin{itemize}
  \tightlist
  \item
    each agent

    \begin{itemize}
    \tightlist
    \item
      choose randomly another agent
    \item
      the two agents pool and share the money as in model 1
    \end{itemize}
  \end{itemize}
\end{itemize}

\vskip-2mm
\hrule

\vskip4mm

In this implementation, each agent has at least an exchange at each time
step. Some agents can have multiple exchanges if it is chosen by
different agents.

In a more complicate variant, a choosing agent can observe a number of other agents and selects one of them according to some criterion. The main loop can be as follows

\vskip2mm
\noindent\textbf{Model 3}
\vskip1mm
\hrule

\begin{itemize}
\tightlist
\item
  at each time step

  \begin{itemize}
  \tightlist
  \item
    each agent

    \begin{itemize}
    \tightlist
    \item
      choose a set of other agents
    \item
      choose one agent in the set according to some preference criterion
    \item
      the two agents pool and share the money as before
    \end{itemize}
  \end{itemize}
\end{itemize}

\vskip-2mm
\hrule

\vskip4mm

Model 1 is so simple that its straightforward implementation does not require an agent-based approach (see Section \ref{sec:model1withoutagents}).

Because we are interested in agent-based models, we will proceed with the agent-based implementation as a tutorial towards more realistic and advanced models.  

In particular, the final goal of this document is to start dealing with parallel implementations of the models. 

Among the many available platforms we will use Repast (\url{https://repast.github.io}).

\section{The Repast suite}

Repast provides facilities to run agent-based simulation.

It comes in three different toolkits. 
The classic one is ``Repast Simphony'', a java-based toolkit designated for use in personal computer and small clusters.  

The increase in computer availability brought the development team to release the ``Repast for High Performance Computing'' toolkit, that is a ``C++-based distributed agent-based modeling toolkit that is designed for use on large computing clusters and supercomputers.'' 

The most recent toolkit is ``Repast for Python''. ``It is a Python-based distributed agent-based modeling toolkit to apply large-scale distributed ABM methods.'' 

All the mentioned Repast toolkits provide facilities to implement agent-based models such as:
\begin{itemize}
\tightlist
	\item a scheduler to controls the dynamic of the simulation
	\item spatial structures such as grids or continuous 2d space to locate agents
	\item network structures
	\item facilities to handle parameters
	\item facilities to record results
	\item \ldots
\end{itemize}

We plan to use ``Repast for Python'', the most up-to-date toolkit in the Repast family. In addition to the previous mentioned facilities, Repast4py builds some of its functions on mpi4py to ease the management of parallel computation.
However, in some cases we have to use the mpi4py functions directly to achieve our goals. 

In the following section we will deal with four implementations of winners-losers model 1:
\begin{itemize}
	\item WL model without agents coded in python (without Repast functions)
	\item WL model with agents in a no parallel setting (Using Repast facilities excluded those for parallel computation)
	\item WL model with agents in a parallel setting (Using repast facilities excluded those for parallel computation. The parallel computation is implemented using mpi4py functions)
	\item WL model with agents in a parallel setting (Using repast facilities including those for parallel computation.)
\end{itemize}

\section{Model 1 without agents}\label{sec:model1withoutagents}

As mentioned above, the implementation can be performed without agents.
It suffice to create an array of money holdings, take two of them at each iteration and update them according to the rule seen above.

The \verb+01_wl_no_agents.py+ script in the code folder (based on Pietro Terna's code from
\url{https://github.com/terna/winners-losers/blob/main/par2.1Chakraborti.ipynb}) provides such implementation.

Type \verb+python 01_wl_no_agents.py+ in a prompt terminal to run the script.


\section{Model 1 not parallel with agents}

code in \verb+01_wl_not_parallel_with_agents.py+.

\noindent command to run: \verb+python 01_wl_not_parallel_with_agents.py+.

\section{Model 1 parallel with MPI functions}



In model 1 we have an interaction for each time tick. The
closer exercise to the basic model in a parallel setting is to make subsequent interaction
to be performed on different processes (or ranks). Although this is not
a true parallelization, it will serve to understand communication among
ranks.

The algorithm we will implement is the following:

\begin{itemize}
\tightlist
\item
  at each time tick

  \begin{itemize}
  \tightlist
  \item
    choose a rank (say Ra)
  \item
	  Ra chooses a rank (say Rb, note that Rb can be equal to Ra. In this case we will have a local interaction)
  \item
    Ra choose an agent (say ARa)
   \item
	   ARa builds a reference to an agent in Rb (say ARb)
  \item
    if Ra == Rb the interaction is local
    \begin{itemize}
		    \tightlist
    \item
	    check that ARb and ARa are different, if not, change the second agent with a different one
    \item
      pool the two agents' money holdings and reallocate the sum randomly
    \end{itemize}
  \item
    else

    \begin{itemize}
    \tightlist
    \item
    Ra sends to all other ranks the following information: 1) the selected agent (Rb), 2) its rank number, and that 3) the interaction is not local (MPI send)
    \item
	    all other ranks receive the new (MPI receive)
    \item
	    Rb discover he was chosen and sets the chosen flag to allow point-to-point communications
	    
	    Rb gets ARb and learn its money holdings
      %ra requests the agent to rb (the ghost of the agent is created in rank1 say GAR2inR1)
    \item
	    Rb send to Ra the info on ARb's money holdings (MPI send)
%      AR1 interact with GAR2inR1, AR1 changes its variable and take note of AR2 variable
    \item
	    Ra receive the info (MPI receive)

	    Ra pool resources and reallocate them

	    Ra updates ARa money holdings
    \item
	    Ra send information on ARb new money holding
    \item
	    Rb receive the information (MPI receive)

	    Rb update ARb's money holding
    \end{itemize}
  \end{itemize}
\end{itemize}

An implementation of the algorithm is provided in the python script \verb+01_wl.py+.

The command
\verb+mpirun -n 4 python 01_wl.py+ executes the model in 4 ranks.
By inserting several print statements, we can verify how the code works on the 4 ranks. The output from time tick 1 and 2 is reported hereafter.


\footnotesize
\begin{verbatim}
RANK 1                                     RANK 2
-- tick 1 ----------------------------------------------------------------------------
updating rank 1 at tick 1 active rank 1
local interaction in rank  1:          
      agent  2  interacts with agent  0    tick 1 active r 1 self r 2 
                                                    received data ((0, 0, 1), 1, False)
sum of money holdings  2
share  0.06755543644766471
money to local agent  0.135  
    money to second local agent  1.864 
The two local agents money was updated 
-- tick 2 ----------------------------------------------------------------------------
                                           updating rank 2 at tick 2 active rank 2
                                           local agent (3, 0, 2) interact with (2, 0, 1)
tick 2 active r 2 self r 1 
       received data ((2, 0, 1), 2, True)
this agent is in my rank, talking with rank 2
information sent to rank 2 money 0.135
                                           information received from rank 1: money 0.135
                                           sum of money holdings  1.135
                                           share  0.86
                                           money to local agent  0.976
                                           money to remote agent  0.158
                                           local agent's money updated
                                           information send to rank 1 money 0.158
Information received from rank 2 money 0.158
agent  (2, 0, 1)  money updated
--------------------------------------------------------------------------------------


RANK 0                                     RANK 3
-- tick 1 ----------------------------------------------------------------------------
tick 1 active r 1 self r 0                 tick 1 active r 1 self r 3 
      received data ((0, 0, 1), 1, False)  received data ((0, 0, 1), 1, False)
-- tick 2 ----------------------------------------------------------------------------
tick 2 active r 2 self r 0                 tick 2 active r 2 self r 3
      received data ((2, 0, 1), 2, True)   received data ((2, 0, 1), 2, True)
--------------------------------------------------------------------------------------
\end{verbatim}
\normalsize

It is interesting to note that agent 2 in rank 1 is involved in both time tick.
In the first time tick, it is randomly chooses as the acting agent. It chooses agent 0 in the same rank to interact with.
In time tick 2 it is selected by agent 3 in rank 2 as a partner for interaction. Therefore rank 1 send information on agent's 2 money holding to rank 2.
Rank 2 performs the computation and send back information on rank 1's agent's money holdings. Finally, rank 1 updates the agent's state.

We discuss hereafter the code relevant for inter rank communication. They can be found in the step function (see the \verb+01_wl.py+ file).  

In case the agent with which to interact is in another rank, the first communication cannot be of the point-to-point type. In fact, in a point-to-point communication, the sending agent has to specify the target rank, and the receiving agent has to specify the rank from which he is receiving. While the sending rank knows the receiving rank, the receiving rank ignores that it was chosen. Therefore, in the first communication, the sending rank has to inform all the other ranks of its decisions. To achieve this goal, in our implementation, the sending rank prepare a tuple 
\begin{verbatim}
tupleToSend=(counterpartTuple,activeRank,interactionBetweenRanks)
\end{verbatim}
and send it to all the other ranks with the following code:
\begin{verbatim}
for aRank in self.otherRanks:
   self.comm.send(tupleToSend,aRank)
\end{verbatim}
all the other ranks receive the sent tuple. They now know who is the requested agent, the sender rank, and that the interaction is between ranks.
Analyzing the requested agent tuple, each rank can establish if it is involved. The code is as follows:
\begin{verbatim}
data=self.comm.recv(source=activeRank)  #receive data
agTuple=data[0]                         #requested agent tuple
interactionBetweenRanks=data[2]         #learn if there is a between rank exchange
if agTuple[2]==self.rank:               #establish if the rank is involved
    chosenFromOtherRanks=True           #set the involved variable
    talkingWith=data[1]                 #set the rank from which massages will be received
\end{verbatim}
Note that the last line of code will allow point-to-point communication hereinafter.

Next to this, in fact, the rank called to interact gets the selected agent, and send the information directly to the active rank with the following code:
\begin{verbatim}
tmpWL=self.context.agent(agTuple)
self.comm.send(tmpWL.money,talkingWith)
\end{verbatim}             

The active rank receives information:
\begin{verbatim}
recWallet=self.comm.recv(source=counterpartRank)
\end{verbatim}
performs the pooling and the reallocation and send back info to the selected rank
\begin{verbatim}
self.comm.send(remoteAgentMoney,counterpartRank)
\end{verbatim}
Finally, the selected rank receives information, and update its agents money holding:
\begin{verbatim}
newMoney=self.comm.recv(source=talkingWith)
tmpWL.money=newMoney
\end{verbatim}

The first part of the code described above can be implemented in an alternative way that uses another interesting MPI function.
We are referring in particular to the part in which the active rank informs all the other ranks. Above we implemented this part as follows:
\begin{verbatim}
for aRank in self.otherRanks:
   self.comm.send(tupleToSend,aRank)
\end{verbatim}
however, we can avoid the for loop using the MPI broadcast function.
To make it works, the active rank defines the \verb+tupleToSend+ as before. All the other rank define the same variable, setting it to \verb+None+.
Finally, all the rank call the \verb+bcast+ function as follows:
\begin{verbatim}
data=self.comm.bcast(tupleToSend,root=activeRank)
\end{verbatim}
afterward, the code proceed as before.

The version of the code using the \verb+bcast+ function in provided in the \verb+01_wl_bcast.py+ script.

In case of running on 4 ranks, the command is:
\begin{verbatim}
mpirun -n 4 python 01_wl_bcast.py
\end{verbatim}

\section{Model 1 parallel with ghosts}

We will describe hereafter how to parallelize the model using the Repast4py way.

Till now, in case of interaction between two agents in two different ranks, the ranks send and receive information to allow their agents to update their states.

Repast4py is based on a mechanism that add to each rank copies of agents that the rank would had not access to.

A key function for achieving the result is \verb+request_agents()+ belonging to the context module.

The mechanism works as follows:

\begin{itemize}
	\tightlist
	\item 
\verb+request_agents+ sends to the other ranks the ids of agents asked for by a rank
	\item 
The other ranks take the requested agents and ask them to save their current state in a tuple.
	\item 
The collected tuples are send back to the requesting rank
	\item 
		The requesting rank uses the received information to create agents. In particular, the rank creates an agent identical to the one which is in the rank for each received tuple. The newly created agents are called \textbf{ghosts}. They are stored in a dedicated list.
	\item 
		Afterwards, the interaction can be performed locally between the agent and a ghost. When a local agent is called to interact with a ghost, the ghost is taken from the rank's ghost list.
\end{itemize}

To make this mechanism work, the user has to perform two setups.

First, the user has to endow agents with a \verb+save(....)+ function.
Repast allows each rank to collect information from requested agent. In particular it calls an agents' function called \verb+save(....)+.
Therefore, the user has to endow the agent with the save function. This function could not be implemented by the Repast team because it is model specific (agents are different among models, and the tuple returned by \verb+save+ can be shaped only by the modeler).

The second code integration consist in providing a function accessible at rank level whose role is to create new agents given a tuple with the same structure of \verb+save+ return. This function is the second input argument of \verb+request_agent+. The function can be named at the modeler convenience, however, the repast convention is to call it \verb+restore_agent(....)+   

We report even in this case some significant lines of code.

The agent's function save:

\begin{verbatim}
    def save(self) -> Tuple:
        return (self.uid,self.money)
\end{verbatim}

and the \verb+restore_agent+ function:

\begin{verbatim}
def restore_agent(agent_data: Tuple):
    tupleFromSaveFunction=agent_data
    uid=tupleFromSaveFunction[0]
    money=tupleFromSaveFunction[1]
    tmp = WLagent(uid[0],uid[2],money)
    return tmp
\end{verbatim}


In the step function, each rank fills a list of tuples we called \verb+agentsToBeRequested+.
In our model only the active rank has an item in the list. The other ranks have empty lists.

\begin{verbatim}
        agentsToBeRequested=[]
        if self.rank == activeRank:
            .. .. ..
            .. .. ..
            if self.rank == counterpartRank:
            .. .. ..
            .. .. ..
            else:
                interactionBetweenRanks=True
                agentsToBeRequested.append((counterpartTuple,counterpartRank))
        self.context.request_agents(agentsToBeRequested,restore_agent)
\end{verbatim}
The list is then used in the \verb+request_agents+ as the first argument. The second argument being the \verb+restore_agent+ function.

The \verb+request_agents+ function populates two lists. The ghosts list and the ghosted list i.e. the list of local agents having ghosts. 
In our model, in case of between rank interaction, the active rank has one element in the ghost list, while the other rank has one element in the ghosted list.
In the following code, a rank learn it will be involved in the interaction checking the ghosted agents list. Because the ghosted list also record the rank where agents are ghosted, the rank also learn the rank with which it will interact. This will allows point-to-point interaction.

\begin{verbatim}
        if len(self.context._agent_manager._ghosted_agents)>0:
            chosenFromOtherRanks=True
            ghosted_keys=list(self.context._agent_manager._ghosted_agents.keys())
            ghostedAgent=self.context._agent_manager._ghosted_agents[ghosted_keys[0]]
            talkingWith=list(ghostedAgent.ghost_ranks.keys())[0]
\end{verbatim}

The active rank takes the agent from the ghost list, gets the money level of the ghost, and decides the new allocation:
\begin{verbatim}
otherAgent=self.context.ghost_agent(counterpartTuple)     
moneySum=aWL.money+otherAgent.money
.. .. ..
.. .. ..
\end{verbatim}
The two agents' money holdings are updated with MPI functions as in the previous case. We cannot use Repast functions because it does not provide function that updates the agents using the ghost status.

Finally, in this version we have to clear both the ghost and the ghosted list
\begin{verbatim}
        if chosenFromOtherRanks:
            .. .. ..
            .. .. ..
            #delete ghosted
            self.context._agent_manager.delete_ghosted(localID)
        #delete ghost
        if self.rank == activeRank and self.rank != counterpartRank:
            self.context._agent_manager.delete_ghost(otherAgent.uid)
\end{verbatim}

\section{Model 2 parallel implementation with ghosts}
Model 2 is characterized by the fact that multiple agents perform interaction at each time step.
We will give first an implementation in which agents in a single rank perform the interaction at each time step. Finally, we provide the full parallel implementation where the agents in all ranks perform the interaction at each time step.
\subsection{One rank at a time}
The algorithm can be as follow:

\begin{itemize}
\tightlist
\item
  Store information on how many ranks there are and how many agents are
  in each rank
\item
	at each time step, take a rank

  \begin{itemize}
  \tightlist
  \item
    create RL1 (an empty list where to store references to agents to be requested to
    other ranks)
  \item
    create RL2 (an empty list where to store info to update original agents in other ranks after local agents interact with ghosts)
  \item
    each agent

    \begin{itemize}
    \tightlist
    \item
	    randomly draw a rank (the counterpartRank)
    \item
      randomly draw an agent id belonging to the rank chosen in the
      previous step
    \item
      if counterpartRank is different from current rank add id to RL1, but only if
      the id is not yet in RL1 (this avoid to request two time the same agent). 
    \item else: avoid self interactions
    \end{itemize}
  \item
    current rank request RL1. As a result current rank create ghosts and
    other ranks accounts for ghosted
  \item
	  each agent: perform interaction
	  \begin{itemize}
		  \item if counterpartRank is equal to current rank: perform local interaction and update money holdings of the two local agents
		  \item else: perform interaction with ghost. Update the local and the ghost money variable.
	  \end{itemize}
	\item current rank perform a loop on the ghosts list to fill RL2 with ghost id and money holding
	\item current rank broadcast RL2 
	\item ranks different from current update original agents money holding using RL2
	\item each rank clear ghost and ghosted lists

  \end{itemize}
\end{itemize}

This model represents a significant change from the previous ones. In the models above, the interaction is managed by the rank, while in the present one the interaction is managed by the agent.  
Therefore, each agent has to be able to access its rank variables such as the list of agents, the list of ghost, the list of messages to send to other ranks. 

To this aim we put these three lines after the includes:
\begin{verbatim}
context = None
agentsToBeRequested=[]
messagesAfterInteraction=[]
\end{verbatim}
The context variable is then assigned when the Model is created:
\begin{verbatim}
        global context
        context = ctx.SharedContext(comm)
\end{verbatim}
In this way, agents can directly add elements to the \verb+agentsToBeRequested+ list, get local agents or ghosts and add to \verb+messagesAfterInteraction+.

The \verb+WLagent+ class now has two more functions: \verb+set_counterpart()+ and \verb+interact()+. 

At each interaction, the acting agent updates both its money holding and its counterpart's money holding either if it is a local agent or a ghost.

Note that the original agents are updated at the end of all the interactions. The list of messages to be send is filled with information taken from the ghosts.

This allows ghosts, like local agents, be involved in several interactions. In this version of the model all the agents of the current rank has at least one interaction.


\subsection{All ranks at a time}
This is a full parallel implementation of model 2 with Repast4py ghosts.
A possible algorithm unfolds as follows:
\begin{itemize}
\tightlist
\item
  Store information on how many ranks there are and how many agents are
  in each rank
\item
  in each rank at each time step

  \begin{itemize}
  \tightlist
  \item
    create RL1 (an empty list where to store references to agents to be requested to other ranks)
  \item
    create RL2 (an empty list where to store info to update original agents in other ranks after local agents interact with ghosts)
  \item
    create RL3 (an empty list where to store references to ghosts to be removed from other ranks)
  \item
    each agent

    \begin{itemize}
    \tightlist
    \item
	    randomly draw a rank (the counterpartRank)
    \item
      if counterpartRank is equal to current rank choose a local agent avoiding self interactions
    \item else
       \begin{itemize}
	    \item
	      randomly draw an agent id belonging to the rank chosen in the
	      previous step
	    \item
	      add the agent's id to RL1, but only if the id is not yet in RL1 (this avoid to request two time the same agent). 
       \end{itemize}
    \end{itemize}
  \item
    all ranks request RL1. As a result ranks create ghosts and ghosted lists
  \item 
	  all ranks analyze the ghosted list and identify ghosts to be removed (this is because each agent has to be ghosted at most in one rank). The IDs of the ghost to be removed are recorded in the RL3 list
  \item 
	  all ranks receive RL3 and remove ghosts using information therein
  \item
	  each agent: perform interaction
	  \begin{itemize}
		  \item if counterpartRank is equal to current rank: perform local interaction and update money holdings of the two local agents
		  \item else:
			  \begin{itemize}
				  \item if counterpart id is not None: perform interaction with ghost. Update the local and the
					  ghost money variable
				  \item else: agent does not interact
			  \end{itemize}
	  \end{itemize}
	\item perform a loop on the ghosts list to fill RL2 with ghost id and money holding
	\item all ranks broadcast RL2 (alltoall()) 
	\item all ranks update original agents money holding using RL2
	\item each rank clear ghost and ghosted lists
  \end{itemize}
\end{itemize}

In the model of the previous section, the following if conditions
\begin{verbatim}
        tick=self.runner.tick()
        activeRank=tick % self.size
        .. .. ..
        if self.rank == activeRank:
\end{verbatim}
makes a rank updating at a time. We remove all the if statement to achieve the present full parallel implementation.

The most important canniness in a full parallel implementation is that an agent must have at most one ghost. 
In fact, if we had multiple ghosts, each of them would have its own history and we might end up with completely different ghosts of the same agent. We would therefore have to choose one of the ghosts to update the real agent. This choice would be difficult to make. Moreover, if it were possible to choose one of the ghosts, the other must be dropped, but they would have changed the history of other agents belonging to the ranks they were in. Therefore, to avoid distortion, we should undo the effect of the changes caused by the dropped ghosts. For these reason we think it convenient to allow for a unic ghost.
The ghosted list can help in identifying if an agent is ghosted in more than a rank. 
%We have to delete ghost in such a way that the agent has only one ghost.

As before, the ghosted list is created by the \verb+request_agents(.. ..)+ function.
Afterward, communication among ranks is mainly performed with the mpi function \verb+alltoall()+. Infact, if all ranks update at each time step this function is particularly useful. For a correct use, we have to learn how to setup its argument. The following code display the argument of the function (i.e. the data sent) and its returned structure (i.e. the data received by ranks).

The data to send is an array of arrays. 
\begin{verbatim}
rank 0 IDs to send to remove ghosts [[], [], [(0, 0, 0)], []]
rank 1 IDs to send to remove ghosts [[], [], [], []]
rank 2 IDs to send to remove ghosts [[], [], [], [(0, 0, 2), (2, 0, 2)]]
rank 3 IDs to send to remove ghosts [[], [], [], []]
\end{verbatim}
Each array contains local agents IDs. The position of the array signal the rank to whom the array will be send.
The first line of code shows that rank 0 will send agent (0,0,0) to rank 2. The third line shows that rank 2 will send two of its agents to rank 3.

The received data is also an array of array.
\begin{verbatim}
rank 0 IDs received to remove ghosts [[], [], [], []]
rank 1 IDs received to remove ghosts [[], [], [], []]
rank 2 IDs received to remove ghosts [[(0, 0, 0)], [], [], []]
rank 3 IDs received to remove ghosts [[], [], [(0, 0, 2), (2, 0, 2)], []]
\end{verbatim}
Now the position of the array signals the source rank of the agents. Therefore, the third line says that rank 2 receives agent (0,0,0) from rank 0. The fourth line says that rank 3 receives two agents from rank 2.

In a sense, received data is a transpose of sent data.

The following code fill the data structure each rank will send:
\begin{verbatim}
        global ghostsToBeRemoved
        ghostsToBeRemoved=[[] for i in range(self.size)]
        ghosted_keys=list(context._agent_manager._ghosted_agents.keys())
        for aKey in ghosted_keys:
            ghostedAgent=context._agent_manager._ghosted_agents[aKey]
            talkingWith=list(ghostedAgent.ghost_ranks.keys())
            if len(talkingWith)>1:
                ghostRanksInExcess=talkingWith[1:len(talkingWith)]
                for aRank in ghostRanksInExcess:
                    ghostsToBeRemoved[aRank].append(aKey)
\end{verbatim}

The array of arrays is named \verb+ghostsToBeRemoved+. For each ghosted agent, the dictionary \verb+ghost_ranks+ lists the rank where the agent is ghosted. If this dictionary has more than one element, we select the element at position higher than zero as the rank where the ghosts of this agents will be deleted.

Once the arrays are ready the \verb+alltoall()+ function is called:
\begin{verbatim}
      data=self.comm.alltoall(ghostsToBeRemoved)
\end{verbatim}
all ranks receive the data, and proceed to remove the ghosts with the following code:
\begin{verbatim}
        for rank, recvTuples in enumerate(data):
            if recvTuples:
                for aTuple in recvTuples:
                    context._agent_manager.delete_ghost(aTuple)
\end{verbatim}


This code ensures ghosts uniqueness, therefore exchanges can be performed as in the previous model: money holdings are accounted
immediately after each exchange both for local agents and ghosts.

The original agents are then updated by collecting information from the ghost list and filling the \verb+messagesAfterInteraction+ list.

These lists are then broadcast to other ranks using again the \verb+alltoall()+ function.

The receiving rank updates original agents money holdings using the information received.

   

\end{document}
