\documentclass{article}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    allcolors=blue
    }
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\begin{document}
\section{Winners-losers models}
In the Winners-Losers model, agents have only one variable: their money holding. Their holdings are modified as follows. At each iteration two agents meet, pool their money holdings, and share the sum randomly.  
Therefore, after an interaction both the agents have their money holdings changed.

The algorithm of the Winners-Losers model is as follows:

\vskip2mm
\noindent\textbf{Model 1}
\vskip1mm
\hrule

\begin{itemize}
\tightlist
\item
  create a population of agents
\item
  initialize their money holding to a given amount (say 1)
\item
  at each time

  \begin{itemize}
  \tightlist
  \item
    two agents are randomly drawn from the population
  \item
    they pool their money holdings
  \item
    the pooled money is reassigned to the two agents giving a random
    share to the first agent and the rest to the other agent.
  \end{itemize}
\end{itemize}
\vskip-2mm
\hrule

\vskip4mm
This dynamics is very simple, but we can have some extreme cases. For
example, an agent can never be chosen.

If we want to avoid these cases, we can modify our main loop as follows

\vskip2mm
\noindent\textbf{Model 2}
\vskip1mm
\hrule

\begin{itemize}
\tightlist
\item
  at each time step

  \begin{itemize}
  \tightlist
  \item
    each agent

    \begin{itemize}
    \tightlist
    \item
      choose randomly another agent
    \item
      the two agents pool and share the money as in model 1
    \end{itemize}
  \end{itemize}
\end{itemize}

\vskip-2mm
\hrule

\vskip4mm

In this implementation, each agent has at least an exchange at each time
step. Some agents can have multiple exchanges if it is chosen by
different agents.

In a more complicate variant, a choosing agent can observe a number of other agents and selects one of them according to some criterion. The main loop can be as follows

\vskip2mm
\noindent\textbf{Model 3}
\vskip1mm
\hrule

\begin{itemize}
\tightlist
\item
  at each time step

  \begin{itemize}
  \tightlist
  \item
    each agent

    \begin{itemize}
    \tightlist
    \item
      choose a set of other agents
    \item
      choose one agent in the set according to some preference criterion
    \item
      the two agents pool and share the money as before
    \end{itemize}
  \end{itemize}
\end{itemize}

\vskip-2mm
\hrule

\vskip4mm

Model 1 is so simple that its straightforward implementation does not require an agent-based approach (see Section \ref{sec:model1withoutagents}).

Because we are interested in agent-based models, we will proceed with the agent-based implementation as a tutorial towards more realistic and advanced models.  

In particular, the final goal of this document is to start dealing with parallel implementations of the models. 

Among the many available platforms we will use Repast (\url{https://repast.github.io}).

\section{The Repast suite}

Repast provides facilities to run agent-based simulation.

It comes in three different toolkits. 
The classic one is ``Repast Simphony'', a java-based toolkit designated for use in personal computer and small clusters.  

The increase in computer availability brought the development team to release the ``Repast for High Performance Computing'' toolkit, that is a ``C++-based distributed agent-based modeling toolkit that is designed for use on large computing clusters and supercomputers.'' 

The most recent toolkit is ``Repast for Python''. ``It is a Python-based distributed agent-based modeling toolkit to apply large-scale distributed ABM methods.'' 

All the mentioned Repast toolkits provide facilities to implement agent-based models such as:
\begin{itemize}
\tightlist
	\item a scheduler to controls the dynamic of the simulation
	\item spatial structures such as grids or continuous 2d space to locate agents
	\item network structures
	\item \ldots
\end{itemize}

We plan to use ``Repast for Python'', the most up-to-date toolkit in the Repast family. In addition to the previous mentioned facilities, Repast4py builds some of its functions on mpi4py to ease the management of parallel computation.

In the following section we will deal with four implementations of winners-losers model 1:
\begin{itemize}
	\item WL model without agents coded in python (without Repast functions)
	\item WL model with agents in a no parallel setting (Using Repast facilities excluded those for parallel computation)
	\item WL model with agents in a parallel setting (Using repast facilities excluded those for parallel computation. The parallel computation is implemented using mpi4py functions)
	\item WL model with agents in a parallel setting (Using repast facilities including those for parallel computation.)
\end{itemize}

\section{Model 1 without agents}\label{sec:model1withoutagents}

As mentioned above, the implementation can be performed without agents.
It suffice to create an array of money holdings, take two of them at each iteration and update them according to the rule seen above.

The \verb+01_wl_no_agents.py+ script in the code folder (based on Pietro Terna's code from
\url{https://github.com/terna/winners-losers/blob/main/par2.1Chakraborti.ipynb}) provides such implementation.

\section{Model 1 not parallel with agents}

code in \verb+01_wl_not_parallel_with_agents.py+.

command to run: \verb+python 01_wl_not_parallel_with_agents.py+.

\section{Model 1 parallel with MPI functions}



In model 1 we have an interaction for each time tick. The
closer exercise to the basic model in a parallel setting is to make subsequent interaction
to be performed on different processes (or ranks). Although this is not
a true parallelization, it will serve to understand communication among
ranks.

The algorithm we will implement is the following:

\begin{itemize}
\tightlist
\item
  at each time tick

  \begin{itemize}
  \tightlist
  \item
    choose a rank (say Ra)
  \item
	  Ra chooses a rank (say Rb, note that Rb can be equal to Ra. In this case we will have a local interaction)
  \item
    Ra choose an agent (say ARa)
   \item
	   ARa builds a reference to an agent in Rb (say ARb)
  \item
    if Ra == Rb the interaction is local
    \begin{itemize}
		    \tightlist
    \item
	    check that ARb and ARa are different, if not change the second agent with a different one
    \item
      pool the two agents' money holdings and reallocate the sum randomly
    \end{itemize}
  \item
    else

    \begin{itemize}
    \tightlist
    \item
	    Ra informs all other ranks that he selected Rb, its rank number and that the interaction is not local (MPI send)
    \item
	    all other ranks receive the new (MPI receive)
    \item
	    Rb discover he was chosen and sets the chosen flag to allow point-to-point communications
	    
	    Rb gets ARb and extract its money holdings
      %ra requests the agent to rb (the ghost of the agent is created in rank1 say GAR2inR1)
    \item
	    Rb send to Ra the info on ARb's money holdings (MPI send)
%      AR1 interact with GAR2inR1, AR1 changes its variable and take note of AR2 variable
    \item
	    Ra receive the info (MPI receive)

	    Ra pool resources and reallocate them

	    Ra updates ARa money holdings
    \item
	    Ra send information on ARb new money holding
    \item
	    Rb receive the information (MPI receive)

	    Rb update ARb's money holding
    \end{itemize}
  \end{itemize}
\end{itemize}

An implementation of the algorithm is provided in the python script \verb+01_wl.py+.

The command
\verb+mpirun -n 4 python 01_wl.py+ executes the model in 4 ranks.
By inserting several print statements, we can verify how the code works on the 4 ranks. The output from time tick 1 and 2 is reported hereafter.


\footnotesize
\begin{verbatim}
RANK 1                                     RANK 2
-- tick 1 ----------------------------------------------------------------------------
updating rank 1 at tick 1 active rank 1
local interaction in rank  1:          
      agent  2  interacts with agent  0    tick 1 active r 1 self r 2 
                                                    received data ((0, 0, 1), 1, False)
sum of money holdings  2
share  0.06755543644766471
money to local agent  0.135  
    money to second local agent  1.864 
The two local agents money was updated 
-- tick 2 ----------------------------------------------------------------------------
                                           updating rank 2 at tick 2 active rank 2
                                           local agent (3, 0, 2) interact with (2, 0, 1)
tick 2 active r 2 self r 1 
       received data ((2, 0, 1), 2, True)
this agent is in my rank, talking with rank 2
information sent to rank 2 money 0.135
                                           information received from rank 1: money 0.135
                                           sum of money holdings  1.135
                                           share  0.86
                                           money to local agent  0.976
                                           money to remote agent  0.158
                                           local agent's money updated
                                           information send to rank 1 money 0.158
Information received from rank 2 money 0.158
agent  (2, 0, 1)  money updated
--------------------------------------------------------------------------------------


RANK 0                                     RANK 3
-- tick 1 ----------------------------------------------------------------------------
tick 1 active r 1 self r 0                 tick 1 active r 1 self r 3 
      received data ((0, 0, 1), 1, False)  received data ((0, 0, 1), 1, False)
-- tick 2 ----------------------------------------------------------------------------
tick 2 active r 2 self r 0                 tick 2 active r 2 self r 3
      received data ((2, 0, 1), 2, True)   received data ((2, 0, 1), 2, True)
--------------------------------------------------------------------------------------
\end{verbatim}
\normalsize

It is interesting to note that agent 2 in rank 1 is involved in both time tick.
In the first time tick, it is randomly chooses as the acting agent. It chooses agent 0 in the same rank to interact with.
In time tick 2 it is selected by agent 3 in rank 2 as a partner for interaction. Therefore rank 1 send information on agent's 2 money holding to rank 2.
Rank 2 performs the computation and send back information on rank 1's agent's money holdings. Finally, rank 1 updates the agent's state.

We highlight here the most relevant lines of code. They are contained in the step function.  

In case the agent with which to interact is in another rank, the first communication cannot be of the point-to-point type. In fact, in a point-to-point communication, the sending agent has to specify the target rank, and the receiving agent has to specify the rank from which he is receiving. While the sending rank knows the receiving rank, the receiving rank ignores that it was chosen. Therefore, in the first communication, the sending rank has to inform all the other rank of its decisions. To achieve this goal, the sending rank prepare a tuple 
\begin{verbatim}
tupleToSend=(counterpartTuple,activeRank,interactionBetweenRanks)
\end{verbatim}
and send it to all the other ranks
\begin{verbatim}
for aRank in self.otherRanks:
   self.comm.send(tupleToSend,aRank)
\end{verbatim}
all the other ranks receive the sent tuple. They now know which is the requested agent, the sender rank, and that the interaction is between ranks.
Analyzing the requested agent tuple, each rank can establish if it is involved. The code is as follows:
\begin{verbatim}
data=self.comm.recv(source=activeRank)  #receive data
agTuple=data[0]                         #agent tuple
interactionBetweenRanks=data[2]         #learn if thre is a between rank exchange
if agTuple[2]==self.rank:               #establish if the rank is involved
    chosenFromOtherRanks=True           #set the involved variable
    talkingWith=data[1]                 #set the rank from which massages will be received
\end{verbatim}
Note that the last line of code will allow point-to-point communication hereinafter.

Next to this, in fact, the rank called to interact gets the selected agent, and send the information directly to the active rank with the following code:
\begin{verbatim}
tmpWL=self.context.agent(agTuple)
self.comm.send(tmpWL.money,talkingWith)
\end{verbatim}             

The active rank receives information:
\begin{verbatim}
recWallet=self.comm.recv(source=counterpartRank)
\end{verbatim}
performs the pooling and the reallocation and send back info to the selected rank
\begin{verbatim}
self.comm.send(remoteAgentMoney,counterpartRank)
\end{verbatim}
Finally, the selected rank receives information, and update its agents money holding:
\begin{verbatim}
newMoney=self.comm.recv(source=talkingWith)
tmpWL.money=newMoney
\end{verbatim}

The first part of the code described above can be implemented in an alternative way that uses another interesting MPI function.
We are referring in particular to the part in which the active rank informs all the other ranks. Above we implemented this part as follows:
\begin{verbatim}
for aRank in self.otherRanks:
   self.comm.send(tupleToSend,aRank)
\end{verbatim}
however, we can avoid the for loop using the MPI broadcast function.
To make it works, the active rank defines the \verb+tupleToSend+ as before. All the other rank define the same variable, setting it to \verb+None+.
Finally, all the rank call the \verb+bcast+ function as follows:
\begin{verbatim}
data=self.comm.bcast(tupleToSend,root=activeRank)
\end{verbatim}
afterward, the code proceed as before.

The version of the code using the \verb+bcast+ function in provided in the \verb+01_wl_bcast.py+ script.

In case of running on 4 ranks, the command is:
\begin{verbatim}
mpirun -n 4 python 01_wl_bcast.py
\end{verbatim}

\section{Model 1 parallel with ghosts}

We will describe hereafter how to parallelize the model using Repast4py in full.

Till now, in case of interaction between two agents in two different ranks, the ranks send and receive information to allow their agents to update their states.

Repast4py is based on a mechanism that add to each rank copies of agents that the rank would had not access to.

A key function for achieving the result is \verb+request_agents()+ belonging to the context module.

The mechanism works as follows:

\begin{itemize}
	\tightlist
	\item 
\verb+request_agents+ sends to the other ranks the ids of agents asked for by a rank
	\item 
The other ranks take the requested agents and ask them to save their current state in a tuple.
	\item 
The collected tuples are send back to the requesting rank
	\item 
		The requesting rank uses the received information to create agents. In particular, the rank creates an agent identical to the one which is in the rank for each received tuple. The newly created agents are called ghosts. They are stored in a dedicated list.
	\item 
		Afterwards, the interaction can be performed locally between the agent and a ghost. The ghost is taken from the rank's ghost list.
\end{itemize}

To make this mechanism work, the user has to perform two setups.

First, the user has to endow agents with a \verb+save(....)+ function.
Repast allows each rank to collect information from requested agent. In particular it calls an agents' function called \verb+save(....)+.
Therefore, the user has to endow the agent with the save function. This function could not be implemented by the Repast team because it is model specific (agents are different among models, and the tuple returned by \verb+save+ can be shaped only by the modeler).

The second code integration consist in providing a function accessible at rank level whose role is to create new agents given a tuple with the same structure of \verb+save+ return. This function is the second input argument of \verb+request_agent+. The function can be named at the modeler convenience, however, the repast convention is to call it \verb+restore_agent(....)+   

We report even in this case some significant lines of code.

The agent's function save:

\begin{verbatim}
    def save(self) -> Tuple:
        return (self.uid,self.money)
\end{verbatim}

and the \verb+restore_agent+ function:

\begin{verbatim}
def restore_agent(agent_data: Tuple):
    tupleFromSaveFunction=agent_data
    uid=tupleFromSaveFunction[0]
    money=tupleFromSaveFunction[1]
    tmp = WLagent(uid[0],uid[2],money)
    return tmp
\end{verbatim}


In the step function, each rank fills a list of tuples we called \verb+agentsToBeRequested+.
In our model only the active rank has an item in the list. The other ranks have empty lists.

\begin{verbatim}
        agentsToBeRequested=[]
        if self.rank == activeRank:
            .. .. ..
            .. .. ..
            if self.rank == counterpartRank:
            .. .. ..
            .. .. ..
            else:
                interactionBetweenRanks=True
                agentsToBeRequested.append((counterpartTuple,counterpartRank))
        self.context.request_agents(agentsToBeRequested,restore_agent)
\end{verbatim}
The list is then used in the \verb+request_agents+ as the first argument. The second argument being the \verb+restore_agent+ function.

The \verb+request_agents+ function populates two lists. The ghosts list and the ghosted list i.e. the list of local agents having ghosts. 
In our model, in case of between rank interaction, the active rank has one element in the ghost list, while the other rank has one element in the ghosted list.
In the following code, a rank learn it will be involved in the interaction checking the ghosted agents list. Because the ghosted list also record the rank where agents are ghosted, the rank also learn the rank with which it will interact. This will allows point-to-point interaction.

\begin{verbatim}
        if len(self.context._agent_manager._ghosted_agents)>0:
            chosenFromOtherRanks=True
            ghosted_keys=list(self.context._agent_manager._ghosted_agents.keys())
            ghostedAgent=self.context._agent_manager._ghosted_agents[ghosted_keys[0]]
            talkingWith=list(ghostedAgent.ghost_ranks.keys())[0]
\end{verbatim}
 


\section{Model 2 parallel implementation with ghosts}
A parallel implementation of model 2 with Repast4py ghosts can be as follows:

\begin{itemize}
\tightlist
\item
  Store information on how many ranks there are and how many agents are
  in each rank
\item
  in each rank at each time step

  \begin{itemize}
  \tightlist
  \item
    create RL1 (an empty list where to store agents to be requested to
    other ranks)
  \item
    each agent

    \begin{itemize}
    \tightlist
    \item
      create AL1 (an empty list where to store ids of agents that will
      be chosen in next 2 steps)
    \item
      randomly draw a rank (or n ranks)
    \item
      randomly draw an agent id belonging to each rank chosen in the
      previous step
    \item
      add agents ids to AL1
    \item
      if rank is different from current rank add id to RL1, but only if
      the id is not yet in RL1 (this is because we will explain below
      that a ghosted agent can be involved at most in an exchange)
    \end{itemize}
  \item
    current rank request RL1. As a result current rank create ghosts and
    other ranks accounts for ghosted
  \end{itemize}
\end{itemize}

Now we have to make sure that each ghost is involved in at most a
transaction. The ghosted list can help in identifying if an agent is
ghosted in more than a rank. We have to delete ghost in such a way that
the agent has only one ghost.

Now exchanges can be performed. Local exchanges can be accounted
immediately exchanges with ghost are registered in a list

The rank send the list and the receiving rank update the agent holding
\end{document}
